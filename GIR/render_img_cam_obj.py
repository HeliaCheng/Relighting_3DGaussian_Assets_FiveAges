import numpy as np
from plyfile import PlyData, PlyElement
import os
import torch
from torch import nn
from utils.graphics_utils import BasicPointCloud
from scene import GaussianModel
import cv2
from gaussian_renderer import render
from scene.cameras import Camera
import torch.nn.functional as F  
import math
from PIL import Image
import cv2
from math import pi, atan2, asin
import imageio.v3 as iio
import torch
import numpy as np
import os
from PIL import Image
from typing import List, Dict, Tuple
from scipy.spatial.transform import Rotation as R
from arguments import OptimizationParams, ParamGroup, PipelineParams
import argparse
from utils.graphics_utils import getWorld2View2, getWorld2View, getProjectionMatrix
from tqdm import tqdm

class GaussianRenderer:
    def __init__(self, opt, scene_dir="./FA_scene", sh_degree=0):
        """
        :param opt: OptimizationParams or other config object
        :param scene_dir: æ–‡ä»¶å¤¹åŒ…å« ply æ¨¡å‹
        :param sh_degree: çƒè°é˜¶æ•°ï¼ˆä¿ç•™ï¼‰
        """
        self.opt = opt
        self.scene_dir = scene_dir
        self.sh_degree = sh_degree
        self.scene_list = []
        self.loaded = False
        self.all_points = None
        self.space = None

    def load_models(self, target="confroom_hl.ply"):
            """åŠ è½½æŒ‡å®šç›®å½•ä¸‹åŒ…å« target çš„ ply æ¨¡å‹ï¼Œå¹¶æ—‹è½¬é¡¶ç‚¹ä½¿Upä»+Zè½¬å‘-Y"""
            for filename in os.listdir(self.scene_dir):
                file_path = os.path.join(self.scene_dir, filename)
                if os.path.isfile(file_path) and target in filename:
                    gaussian_model = GaussianModel(sh_degree=self.sh_degree)
                    
                    gaussian_model.load_ply_safe(file_path)
                   
                    self.scene_list.append(gaussian_model)

            if not self.scene_list:
                raise ValueError("No Gaussian models loaded. scene_list is empty.")
            self.loaded = True
            print(f"Successfully loaded {len(self.scene_list)} valid Gaussian models")
            return self

    def render_cam_and_img(self,
                           fibo_views: int,
                           output_base_dir: str,
                           pipe,
                           radius: float = None,
                           ring_count: int = 100):
        """
        ç”Ÿæˆä¸¤ç»„è§†è§’å¹¶æ¸²æŸ“ï¼Œåˆ†åˆ«å­˜å‚¨åˆ°ä¸åŒæ–‡ä»¶å¤¹ï¼š
          1) Fibonacci çƒé¢é‡‡æ ·ï¼ˆæ•°é‡ fibo_viewsï¼‰
          2) ç¯ç»•ä¸­ç‚¹é«˜åº¦çš„ä¸€åœˆé‡‡æ ·ï¼ˆæ•°é‡ ring_countï¼Œé»˜è®¤100ï¼‰
        
        :param fibo_views: çƒé¢å‡åŒ€é‡‡æ ·è§†è§’æ•°é‡ï¼ˆå¯ä¸º0è·³è¿‡ï¼‰
        :param output_base_dir: åŸºç¡€è¾“å‡ºç›®å½•ï¼Œå°†åœ¨å…¶ä¸­åˆ›å»ºå­æ–‡ä»¶å¤¹
        :param pipe: pipeline params ç»™ render ä½¿ç”¨
        :param radius: è‹¥ Noneï¼Œä¼šè‡ªåŠ¨è®¾ä¸ºåœºæ™¯ bbox æœ€å¤§è¾¹é•¿çš„ 0.8 å€ï¼›å¦åˆ™ç”¨è¯¥ r
        :param ring_count: ç¯ç»•é‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤100ï¼‰
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        fibo_img_dir = os.path.join(output_base_dir, "fibo", "img_bl")
        fibo_cam_dir = os.path.join(output_base_dir, "fibo", "cam")
        ring_img_dir = os.path.join(output_base_dir, "ring", "img_bl")
        ring_cam_dir = os.path.join(output_base_dir, "ring", "cam")
        
        os.makedirs(fibo_img_dir, exist_ok=True)
        os.makedirs(fibo_cam_dir, exist_ok=True)
        os.makedirs(ring_img_dir, exist_ok=True)
        os.makedirs(ring_cam_dir, exist_ok=True)

        # compute bbox & center
        self.space = self._compute_scene_bounding_box()
        center = np.array([
            (self.space["L"] + self.space["R"]) / 2.0,
            (self.space["D"] + self.space["U"]) / 2.0,
            (self.space["B"] + self.space["F"]) / 2.0
        ], dtype=np.float32)
        print(f"Scene center: {center}")

        # auto radius estimate
        bbox_diag = np.sqrt((self.space["R"] - self.space["L"])**2 +
                            (self.space["U"] - self.space["D"])**2 +
                            (self.space["F"] - self.space["B"])**2)
        if radius is None:
            r = max(0.2, 0.55 * bbox_diag)
        else:
            r = float(radius)
        print(f"Camera radius: {r}")

        # æ¸²æŸ“æ¨¡å‹
        model = self.scene_list[0]
        bg_color_tensor = torch.tensor([0,0,0], dtype=torch.float32, device="cuda")

        # 1) Fibonacci sphere sampling
        if fibo_views > 0:
            print(f"\nğŸ“¸ Rendering Fibonacci sphere views ({fibo_views})...")
            fibo_pts = self._fibonacci_sphere_points(center, r, fibo_views)
            for i, pos in enumerate(tqdm(fibo_pts, desc="Fibonacci views")):
                cam = self._make_camera_from_pos(pos, center, uid=i)
                
                # ä¿å­˜ç›¸æœºå¤–å‚
                R_c2w = cam.R  # å·²ç»æ˜¯ 3x3
                T_w2c = cam.T  # 3x1
                extrinsics=np.eye(4)
                extrinsics[:3,:3]=R_c2w
                extrinsics[:3,3]=T_w2c
                cam_path = os.path.join(fibo_cam_dir, f"cam_{i:03d}.txt")
                np.savetxt(cam_path, extrinsics, fmt="%.6f", delimiter=" ")
                
                # æ¸²æŸ“å›¾åƒ
                render_output = self._render_single_view(cam, model, bg_color_tensor, pipe)
                img_path = os.path.join(fibo_img_dir, f"img_{i:03d}.png")
                self._save_image(render_output, img_path)

        # 2) Ring sampling
        if ring_count > 0:
            print(f"\nğŸ“¸ Rendering ring views ({ring_count})...")
            # æ”¹åŠ¨ï¼šä¼ é€’radius=råˆ°_ring_pointsï¼Œç”¨äº45åº¦è§’è®¡ç®—
            ring_pts = self._ring_points(center, r, ring_count)
            for i, pos in enumerate(tqdm(ring_pts, desc="Ring views")):
                cam = self._make_camera_from_pos(pos, center, uid=i + fibo_views)
                
                # ä¿å­˜ç›¸æœºå¤–å‚
                R_c2w = cam.R  # å·²ç»æ˜¯ 3x3
                T_w2c = cam.T  # 3x1
                extrinsics=np.eye(4)
                extrinsics[:3,:3]=R_c2w
                extrinsics[:3,3]=T_w2c
                cam_path = os.path.join(ring_cam_dir, f"cam_{i:03d}.txt")
                np.savetxt(cam_path, extrinsics, fmt="%.6f", delimiter=" ")
                
                # æ¸²æŸ“å›¾åƒ
                render_output = self._render_single_view(cam, model, bg_color_tensor, pipe)
                img_path = os.path.join(ring_img_dir, f"img_{i:03d}.png")
                self._save_image(render_output, img_path)

        print(f"\nâœ… All images saved. Total views: Fibonacci={fibo_views}, Ring={ring_count}")
        print(f"   Fibonacci views saved to: {fibo_img_dir}")
        print(f"   Ring views saved to: {ring_img_dir}")

    def _render_single_view(self, cam, model, bg_color, pipe):
        """æ¸²æŸ“å•å¼ è§†å›¾"""
        render_output = render(
            viewpoint_camera=cam,
            pc=model,
            bg_color=bg_color,
            pipe=pipe,
            scaling_modifier=1,
            iteration=1,
        )
        
        # æ ¹æ®å®é™…çš„renderè¾“å‡ºé”®åè°ƒæ•´
        if "render" in render_output:
            rgb_tensor = render_output["render"]
        elif "image" in render_output:
            rgb_tensor = render_output["image"]
        else:
            print("Render output keys:", render_output.keys())
            raise KeyError("Can't find image tensor in render output")
        
        return rgb_tensor

    def _save_image(self, rgb_tensor, img_path):
        """ä¿å­˜å›¾åƒ"""
        rgb_tensor = torch.clamp(rgb_tensor, 0.0, 1.0)
        img_np = (rgb_tensor.permute(1, 2, 0).cpu().detach().numpy() * 255).astype(np.uint8)
        img = Image.fromarray(img_np)
        img.save(img_path)

    def _compute_scene_bounding_box(self):
        all_points = []
        for gaussian_data in self.scene_list:
            pts = None
            # å°è¯•å¸¸è§å±æ€§å
            if hasattr(gaussian_data, "_xyz"):
                pts = gaussian_data._xyz
            elif hasattr(gaussian_data, "xyz"):
                pts = gaussian_data.xyz
            elif isinstance(gaussian_data, dict) and "xyz" in gaussian_data:
                pts = gaussian_data["xyz"]

            if pts is None:
                continue

            if isinstance(pts, torch.Tensor):
                pts = pts.detach().cpu().numpy()

            if pts.size == 0:
                continue
            all_points.append(pts)

        if not all_points:
            raise ValueError("self.scene_list ä¸­æ— æœ‰æ•ˆç‚¹åæ ‡æ•°æ®")

        all_points = np.vstack(all_points)
        min_x, min_y, min_z = all_points.min(axis=0)
        max_x, max_y, max_z = all_points.max(axis=0)
        self.all_points = all_points
        return {
            "U": float(max_y),
            "D": float(min_y),
            "L": float(min_x),
            "R": float(max_x),
            "F": float(max_z),
            "B": float(min_z)
        }

    def _fibonacci_sphere_points(self, center: np.ndarray, r: float, n: int):
        """
        ç”Ÿæˆ n ä¸ªåœ¨çƒé¢åŠå¾„ r ä¸Šå‡åŒ€åˆ†å¸ƒçš„ç‚¹ï¼ˆFibonacci sphereï¼‰
        è¿”å› numpy array åˆ—è¡¨ shape (n,3)
        """
        points = []
        if n <= 0:
            return points

        # é»„é‡‘è§’
        phi = np.pi * (3.0 - np.sqrt(5.0))
        for i in range(n):
            z = 1.0 - (i / float(n - 1)) * 2.0  # y ä» 1 åˆ° -1
            radius = np.sqrt(max(0.0, 1.0 - z * z))
            theta = phi * i
            x = np.cos(theta) * radius
            y = np.sin(theta) * radius
            # å•ä½çƒåæ ‡ (x,y,z) -> ç¼©æ”¾ rï¼Œå¹³ç§»ä¸­å¿ƒ
            p = center + r * np.array([x, y, z], dtype=np.float32)
            points.append(p)
        return points

    def _ring_points(self, center: np.ndarray, r: float, n: int):
        """
        ç”Ÿæˆä¸åœ°é¢å¤¹è§’45åº¦çš„ç¯ç»•é‡‡æ ·ç‚¹ï¼ˆä¿¯è§†ç‰©ä½“ï¼‰
        :param center: (3,) åœºæ™¯ä¸­å¿ƒ
        :param r: ç›¸æœºåˆ°åœºæ™¯ä¸­å¿ƒçš„æ¬§å¼è·ç¦»ï¼ˆä¿æŒä¸å˜ï¼‰
        :param n: ç‚¹æ•°é‡
        """
        points = []
        if n <= 0:
            return points
        
        # 45åº¦è§’å‡ ä½•è®¡ç®—ï¼š
        # è®¾ç›¸æœºåˆ°åœºæ™¯ä¸­å¿ƒçš„è¿çº¿ä¸XZå¹³é¢ï¼ˆåœ°é¢ï¼‰å¤¹è§’ä¸º45Â°
        # åˆ™ï¼šyè½´é«˜åº¦å·® = æ°´å¹³è·ç¦»ï¼ˆXZå¹³é¢ï¼‰
        # ç”±å‹¾è‚¡å®šç†ï¼š(æ°´å¹³è·ç¦»)^2 + (é«˜åº¦å·®)^2 = r^2 â†’ 2*(é«˜åº¦å·®)^2 = r^2 â†’ é«˜åº¦å·® = r/âˆš2
        height_offset = r / np.sqrt(2)  # 45åº¦è§’å¯¹åº”çš„é«˜åº¦å·®
        cz = center[2] + height_offset  # ç›¸æœºyåæ ‡ï¼ˆåœ¨åœºæ™¯ä¸­å¿ƒä¸Šæ–¹height_offsetå¤„ï¼‰

        for i in range(n):
            theta = 2.0 * np.pi * (i / float(n))
            # è®¡ç®—XZå¹³é¢ä¸Šçš„åç§»ï¼ˆæ°´å¹³è·ç¦» = r/âˆš2ï¼‰
            x_offset = (r / np.sqrt(2)) * np.cos(theta)
            y_offset = (r / np.sqrt(2)) * np.sin(theta)
            # ç›¸æœºä½ç½®ï¼šåœºæ™¯ä¸­å¿ƒX/Z + æ°´å¹³åç§»ï¼ŒY + é«˜åº¦åç§»
            x = center[0] + x_offset
            y = center[1] + y_offset
            z=cz
            points.append(np.array([x, y, z], dtype=np.float32))
        return points

    def _make_camera_from_pos(self, pos, center, uid=0):
        """
        åŸºäºä¸–ç•Œåæ ‡ pos å’Œç‰©ä½“ä¸­å¿ƒ center æ„é€  3DGS/OpenGL é£æ ¼ç›¸æœº
        åæ ‡ç³»çº¦å®šï¼ˆOpenGL right-hand, -Z çœ‹å‘å‰æ–¹ï¼‰ï¼š
            camera x = right
            camera y = up
            camera z = -forwardï¼ˆcamera çœ‹å‘ -Zï¼‰
        """

        pos = np.asarray(pos, dtype=np.float32)
        print(pos,"pos")
        center = np.asarray(center, dtype=np.float32)
        #RDF
        # 1. ç›¸æœºæœå‘
        forward = center - pos  # æŒ‡å‘ç›®æ ‡
        forward /= np.linalg.norm(forward)

        # world up
        world_up = np.array([0, 0, -1], dtype=np.float32)
        if abs(np.dot(world_up, forward)) > 0.99:
            world_up = np.array([1, 0, 0], dtype=np.float32)

    
        right = np.cross(world_up,forward)
        right /= np.linalg.norm(right)

      
        up = np.cross(forward,right)
        up /= np.linalg.norm(up)

        # 2. æ„é€ æ—‹è½¬çŸ©é˜µ c2wï¼ˆåˆ—å †å ï¼‰
        
        R_c2w = np.stack([right, up, forward], axis=1).astype(np.float32)
        R_w2c = R_c2w.T

        # 3. å¹³ç§»
        T_w2c = -R_w2c @ pos

        # 4. æ„é€  4x4 çŸ©é˜µ
        w2c = np.eye(4, dtype=np.float32)
        w2c[:3, :3] = R_w2c
        w2c[:3, 3] = T_w2c

        c2w = np.eye(4, dtype=np.float32)
        c2w[:3, :3] = R_c2w
        c2w[:3, 3] = pos

        # 5. Camera å¯¹è±¡
        fov_x = fov_y = 90
        img_w = img_h = 1024
        # fx = fy = img_w / 2.0
        # cx = cy = img_w / 2.0

        cam = Camera(
            colmap_id=uid,
            R=R_c2w,
            T=T_w2c,
            FoVx=fov_x,
            FoVy=fov_y,
            gt_alpha_mask=None,
            image_mask=torch.zeros(1, img_h, img_w),
            image=torch.zeros(3, img_h, img_w, device="cuda"),
            image_name=f"view_{uid}",
            uid=uid,
            data_device="cuda"
        )
        camera_center = torch.tensor(pos, dtype=torch.float32, device="cuda")
        trans=[0,0,0]
        #getWorld2View2:input R_c2w(column-first),T_w2c----output:w2c(row-first)
        cam.world_view_transform = torch.tensor(getWorld2View2(R_c2w, T_w2c, trans, scale=1)).transpose(0, 1).cuda()
        cam.projection_matrix = getProjectionMatrix(
                            cam.znear, cam.zfar,cam.FoVx,cam.FoVy).transpose(0, 1).cuda()
        cam.full_proj_transform = (
                        cam.world_view_transform.unsqueeze(0).bmm(cam.projection_matrix.unsqueeze(0))).squeeze(0)
        cam.camera_center = cam.world_view_transform.inverse()[3, :3]
        cam.c2w = cam.world_view_transform.transpose(0, 1).inverse()
        # cam.intrinsics = cam.get_intrinsics()
        # cam.extrinsics = cam.get_extrinsics()
        # cam.proj_matrix = cam.get_proj_matrix()
                    
        assert torch.allclose(cam.camera_center, camera_center, atol=1e-5), \
                        f"[Err] camera_center mismatch {cam.camera_center} vs {camera_center}"
        

        return cam



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    args = parser.parse_args()
    opt = OptimizationParams(parser)
    pipe = PipelineParams(parser)

    renderer = GaussianRenderer(opt=opt, scene_dir="../Relightable3DGaussian/FA_scene/gs_results", sh_degree=0)
    renderer.load_models(target="chair.ply")
    
    sample_fibo = 200  # çƒé¢è§†è§’æ•°
    output_base_dir = "../Relightable3DGaussian/FA_scene/gs_results/chair_bl"  # åŸºç¡€è¾“å‡ºç›®å½•
    
    renderer.render_cam_and_img(
        fibo_views=sample_fibo,
        output_base_dir=output_base_dir,
        pipe=pipe,
        radius=None,    # è‡ªåŠ¨ä¼°è®¡åŠå¾„
        ring_count=100  # ç¯ç»•é‡‡æ ·100å¼ 
    )